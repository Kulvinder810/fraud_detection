# ğŸš€ Real-Time Data Pipeline with Kafka, Spark, and Databricks

A scalable **real-time data pipeline** using **Confluent Kafka** as a message broker, a **Python Producer** to simulate transactions, and a **Databricks Consumer** to process the data.

---

## ğŸ“‚ Project Structure

```
/project-root/
â”‚â”€â”€ producer/           # Python Kafka Producer
â”‚   â”œâ”€â”€ producer.py     # Sends messages to Kafka (Confluent)
â”‚â”€â”€ consumer/           # Databricks Consumer
â”‚   â”œâ”€â”€ consumer.py     # Reads and processes messages
â”‚â”€â”€ config/             # Stores configuration settings
â”‚   â”œâ”€â”€ config.py       # User-defined settings (Kafka, Database)
â”‚â”€â”€ requirements.txt    # Python dependencies
â”‚â”€â”€ README.md           # Project documentation
â”‚â”€â”€ .gitignore          # Ignore unnecessary files
```

---

## ğŸ› ï¸ Prerequisites

Ensure you have the following installed before proceeding:

âœ… **Python 3.8+**  
âœ… **Kafka (Confluent Cloud or Local Setup)**  
âœ… **Databricks Cluster** (For consumer processing)  
âœ… **Confluent Kafka Python Library**  

---

## âš™ï¸ Configuration Setup

Edit the `config.py` file inside the **config/** folder to match your setup.

```python
# Confluent Kafka Configuration
KAFKA_CONFIG = {
    "bootstrap.servers": "your-confluent-bootstrap-url:9092",
    "security.protocol": "SASL_SSL",
    "sasl.mechanisms": "PLAIN",
    "sasl.username": "your-api-key",
    "sasl.password": "your-api-secret",
}

KAFKA_TOPIC = "transactions"

ğŸ”’ **Never hardcode sensitive credentials. Use `.env` or environment variables.**  

---

## ğŸš€ Running the Producer (Python)

1ï¸âƒ£ **Navigate to the producer folder**  
```bash
cd producer
```

2ï¸âƒ£ **Install dependencies**  
```bash
pip install -r ../requirements.txt
```

3ï¸âƒ£ **Run the producer**  
```bash
python producer.py
```

ğŸ“Œ **This script will send transaction data to Kafka every few seconds.**  

---

## ğŸ”¥ Running the Consumer (Databricks)

1ï¸âƒ£ **Upload `consumer.py` to your Databricks workspace**  
2ï¸âƒ£ **Attach it to a Databricks cluster**  
3ï¸âƒ£ **Run the script in a notebook or as a job**  

ğŸ“Œ **The consumer will read Kafka messages and process them in Databricks.**  

---

## ğŸ› Troubleshooting

âŒ **Producer not sending messages?**  
âœ”ï¸ Ensure Kafka is running:  
```bash
confluent login
confluent kafka cluster list
confluent kafka topic list
```
âœ”ï¸ Verify your **Kafka topic exists**:  
```bash
confluent kafka topic describe transactions
```

âŒ **Databricks not receiving messages?**  
âœ”ï¸ Check if the consumer script is properly consuming from Kafka by running it manually in Databricks.


---

## ğŸ’¡ Contributing

ğŸš€ Contributions are welcome! Feel free to submit a **pull request** or open an **issue** if you find any bugs or have feature requests.

---

## ğŸ‘¨â€ğŸ’» Author

ğŸ‘¤ **Kulvinder Singh**  
ğŸ”— [GitHub Profile](https://github.com/Kulvinder810)  
âœ‰ï¸ Contact: kuls810@gmail.com  

---

### ğŸ‰ **Happy Coding! ğŸš€ğŸ”¥**
